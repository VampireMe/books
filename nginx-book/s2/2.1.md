# 2.1 初探 Nginx 架构
***

&emsp;&emsp;
众所周知 Nginx 性能高，而 Nginx 的高性能与其架构是分不开的。
那么 Nginx 究竟是怎么样的呢？
这一节先来初识一下 Nginx 框架。

&emsp;&emsp;
Nginx 在启动后在 Unix 系统中会以 daemon 的方式在后台运行，后台进程包含一个 master 进程和多个 worker 进程。
也可以手动地关掉后台模式让 Nginx 在前台运行，并且通过配置让 Nginx 取消 master 进程，从而可以使 Nginx 以单进程方式运行。
很显然生产环境下肯定不会这么做，所以关闭后台模式一般是用来调试的，在后面的章节里面会详细地讲解如何调试 Nginx。
可以看到 Nginx 是以多进程的方式来工作的，当然 Nginx 也支持多线程的方式，只是主流的方式还是多进程，这也是 Nginx 的默认方式。
Nginx 采用多进程的方式有诸多好处，所以就主要讲解 Nginx 的多进程模式。

&emsp;&emsp;
刚才讲到 Nginx 在启动后会有一个 master 进程和多个 worker 进程。
master 进程主要用来管理 worker 进程，包含：接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态，当 worker 进程退出后 (异常情况下) 会自动重新启动新的 worker 进程。
而基本的网络事件则是放在 worker 进程中来处理。
多个 worker 进程之间是对等的，它们同等竞争来自客户端的请求，各进程互相之间是独立的。
一个请求只可能在一个 worker 进程中处理，一个 worker 进程不可能处理其它进程的请求。
worker 进程的个数是可以设置的，一般会设置与机器 CPU 核数一致，这里面的原因与 Nginx 的进程模型以及事件处理模型是分不开的。
Nginx 的进程模型，可以由下图来表示：

![image](/images/2.1/01.png)

&emsp;&emsp;
在 Nginx 启动后如果要操作 Nginx 应该怎么做呢？
从上文中可以看到，master 来管理 worker 进程，所以只需要与 master 进程通信。
master 进程会接收来自外界发来的信号，再根据信号做不同的事，所以要控制 Nginx 只需通过 kill 向 master 进程发送信号。
比如 kill -HUP pid，是告诉 Nginx 从容地重启，一般用这个信号来重启 Nginx 或重新加载配置，因为是从容地重启，服务是不中断的。
master 进程在接收到 HUP 信号后是怎么做的呢？
master 进程在接到信号后，会先重新加载配置文件，然后再启动新的 worker 进程，并向所有老的 worker 进程发送信号，告诉它们可以光荣退休了。
新的 worker 在启动后就开始接收新的请求，而老的 worker 在收到来自 master 的信号后就不再接收新的请求，并且将当前进程中所有未处理完的请求处理完成后再退出。
当然直接给 master 进程发送信号，这是比较老的操作方式，Nginx 在 0.8 版本之后引入了一系列命令行参数来方便管理。
比如 "./nginx -s reload" 重启 Nginx，"./nginx -s stop" 停止 Nginx 的运行。
如何做到的呢？
还是拿 reload 来说，当执行命令时是启动一个新的 Nginx 进程，而新的 Nginx 进程在解析到 reload 参数后就知道目的是控制 Nginx 来重新加载配置文件，它会向 master 进程发送信号，然后接下来的动作就和直接向 master 进程发送信号一样了。

&emsp;&emsp;
现在知道了操作 Nginx 时，Nginx 内部做了些什么事情，那么 worker 进程又是如何处理请求的呢？
前面有提到 worker 进程之间是平等的，每个进程处理请求的机会也是一样的。
当提供 80 端口的 http 服务时，一个连接请求过来每个进程都有可能处理这个连接，怎么做到的呢？
首先每个 worker 进程都是从 master 进程 fork 过来，在 master 进程里先建立好需要 listen 的 socket(listenfd)之后，再 fork 出多个 worker 进程。
所有 worker 进程的 listenfd 会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢 accept_mutex，抢到互斥锁的那个进程注册 listenfd 读事件，在读事件里调用 accept 接受该连接。
当一个 worker 进程在 accept 这个连接之后，就开始读取请求、解析请求、处理请求，产生数据后，再返回给客户端，最后才断开连接，一个完整的请求就是这样的。
可以看到一个请求完全由 worker 进程来处理，而且只在一个 worker 进程中处理。

&emsp;&emsp;
那么 Nginx 采用这种进程模型有什么好处呢？
首先对于每个 worker 进程，独立的进程不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。
其次采用独立的进程，可以让进程互相之间不会影响，一个进程退出后其它进程还在工作，服务不会中断，master 进程则很快启动新的 worker 进程。
当然 worker 进程的异常退出，肯定是程序有 bug 了，异常退出会导致当前 worker 上的所有请求失败，不过不会影响到所有请求，所以降低了风险。
当然好处还有很多，大家可以慢慢体会。

&emsp;&emsp;
上面讲了很多关于 Nginx 的进程模型，接下来来看看 Nginx 是如何处理事件的。

&emsp;&emsp;
有人可能要问了，Nginx 采用多 worker 的方式来处理请求，每个 worker 里只有一个主线程，那能够处理的并发数很有限啊，多少个 worker 就能处理多少个并发，何来高并发呢？
非也，这就是 Nginx 的高明之处，Nginx 采用了异步非阻塞的方式来处理请求，也就是说 Nginx 是可以同时处理成千上万个请求的。
想想 Apache 的常用工作方式 (apache 也有异步非阻塞版本，但因其与自带某些模块冲突，所以不常用)，每个请求会独占一个工作线程，当并发数上到几千时，就同时有几千的线程在处理请求了。
这对操作系统来说是个不小的挑战，线程带来的内存占用非常大，线程的上下文切换带来的 CPU 开销很大，自然性能就上不去了，而这些开销完全是没有意义的。

&emsp;&emsp;
为什么 Nginx 可以采用异步非阻塞的方式来处理呢，或者异步非阻塞到底是怎么回事呢？
先回到原点，看看一个请求的完整过程。
首先请求过来，要建立连接，然后再接收数据，接收数据后再发送数据。
具体到系统底层，就是读写事件，而当读写事件没有准备好时必然不可操作，如果不用非阻塞的方式来调用，那就得阻塞调用了，事件没有准备好就只能等待，等事件准备好了再继续吧。
阻塞调用会进入内核等待，CPU 就会让出去给别人使用，对于单线程的 worker 显然不合适，当网络事件越多时大家都在等待，CPU 空闲下来没人用，CPU 利用率自然上不去了，更别谈高并发了。
好吧，增加进程数，那这跟 Apache 的线程模型有什么区别，注意别增加无谓的上下文切换。
所以在 Nginx 里最忌讳阻塞的系统调用。
不要阻塞，那就使用非阻塞。
非阻塞就是事件没有准备好，马上返回 EAGAIN，通知事件还没准备好，过会再来吧。
过一会再来检查一下事件直到事件准备好了为止，在这期间就可以先去做其它事情，然后再来看看事件好了没。

&emsp;&emsp;
虽然不阻塞但还得不时地过来检查一下事件的状态，就可以做更多的事情了，但带来的开销也是不小的。
所以才会有了异步非阻塞的事件处理机制，具体到系统调用就是像 select/poll/epoll/kqueue 这样的系统调用。
它们提供了一种机制，可以同时监控多个事件，调用它们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了就返回。
这种机制正好解决了上面的两个问题，拿 epoll 为例 (在后面的例子中，多以 epoll 为例子，以代表这一类函数)，当事件没准备好时，放到 epoll 里，事件准备好了就去读写，当读写返回 EAGAIN 时，将它再次加入到 epoll 里。
这样只要有事件准备好了就去处理它，只有当所有事件都没准备好时才在 epoll 里等待。
这样就可以并发处理大量的并发了，当然这里的并发请求是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好而主动让出的。
这里的切换是没有任何代价，可以理解为循环处理多个准备好的事件。

&emsp;&emsp;
之前说过推荐设置 worker 的个数为 CPU 的核数，在这里就很容易理解了，更多的 worker 数只会导致进程来竞争 CPU 资源，从而带来不必要的上下文切换。
而且 Nginx 为了更好的利用多核特性，提供了 CPU 亲缘性的绑定选项，可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来 cache 的失效。
像这种小的优化在 Nginx 中非常常见，比如 Nginx 在做 4 个字节的字符串比较时，会将 4 个字符转换成一个 int 型再作比较，以减少 CPU 的指令数等等。

&emsp;&emsp;
现在知道 Nginx 为什么会选择这样的进程模型与事件模型了。
对于一个基本的 web 服务器，事件通常有三种类型：网络事件、信号、定时器。
从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决。
如何处理信号与定时器？

&emsp;&emsp;
首先信号的处理。
对 Nginx 有一些特定的信号代表着特定的意义。
信号会中断程序当前的运行，在改变状态后继续执行。
如果是系统调用，则可能会导致系统调用的失败，需要重入。
关于信号的处理，可以学习一些专业书籍，这里不多说。
Nginx 正在等待事件 (epoll_wait) 时，如果程序收到信号，在信号处理函数处理完后，epoll_wait 会返回错误，然后程序可再次进入 epoll_wait 调用。

&emsp;&emsp;
另外再来看看定时器。
由于 epoll_wait 等函数在调用时是可以设置一个超时时间的，所以 Nginx 借助这个超时时间来实现定时器。
Nginx 里的定时器事件是放在一棵维护定时器的红黑树里，每次在进入 epoll_wait 前，先从该红黑树里拿到所有定时器事件的最小时间，在计算出 epoll_wait 的超时时间后进入 epoll_wait。
所以当没有事件产生也没有中断信号时 epoll_wait 会超时，也就是说定时器事件到了。
这时 Nginx 会检查所有的超时事件，将它们的状态设置为超时，然后再去处理网络事件。
由此可以看出当写 Nginx 代码，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件。

&emsp;&emsp;
可以用一段伪代码来总结一下 Nginx 的事件处理模型：

    while (true) {
        for t in run_tasks:
            t.handler();

        update_time(&now);
        timeout = ETERNITY;

        for t in wait_tasks: /* sorted already */
            if (t.time <= now) {
                t.timeout_handler();
            } else {
                timeout = t.time - now;
                break;
            }

        nevents = poll_function(events, timeout);

        for i in nevents:
            task t;
            if (events[i].type == READ) {
                t.handler = read_handler;
            } else { /* events[i].type == WRITE */
                t.handler = write_handler;
            }
            run_tasks_add(t);
    }

&emsp;&emsp;
本节讲了进程模型、事件模型，包括网络事件、信号、定时器事件。