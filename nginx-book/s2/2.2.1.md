# connection
***

&emsp;&emsp;
在 nginx 中 connection 就是对 tcp 连接的封装，其中包括连接的 socket，读事件，写事件。
利用 nginx 封装的 connection，可以很方便的使用 nginx 来处理与连接相关的事情，比如，建立连接，发送与接受数据等。
而 nginx 中的 http 请求的处理就是建立在 connection 之上的，所以 nginx 不仅可以作为一个 web 服务器，也可以作为邮件服务器。
当然利用 nginx 提供的 connection，可以与任何后端服务打交道。

&emsp;&emsp;
结合一个 tcp 连接的生命周期，我们看看 nginx 是如何处理一个连接的。
首先 nginx 在启动时，会解析配置文件，得到需要监听的端口与 ip 地址，然后在 nginx 的 master 进程里，先初始化好这个监控的 socket(创建 socket，设置 addrreuse 等选项，绑定到指定的 ip 地址端口，再listen)，然后再 fork 出多个子进程出来，然后子进程会竞争 accept 新的连接。此时客户端就可以向 nginx 发起连接了。
当客户端与服务端通过三次握手建立好一个连接后，nginx 的某一个子进程会 accept 成功，得到这个建立好的连接的 socket，然后创建 nginx 对连接的封装，即 ngx_connection_t 结构体。
接着设置读写事件处理函数并添加读写事件来与客户端进行数据的交换。
最后 nginx 或客户端来主动关掉连接，到此一个连接就寿终正寝了。

&emsp;&emsp;
当然 nginx 也是可以作为客户端来请求其它 server 的数据的（如 upstream 模块），此时与其它 server 创建的连接也封装在 ngx_connection_t 中。
作为客户端 nginx 先获取一个 ngx_connection_t 结构体，然后创建 socket，并设置 socket 的属性（比如非阻塞）。
然后再通过添加读写事件，调用 connect/read/write 来调用连接，最后关掉连接，并释放 ngx_connection_t。

&emsp;&emsp;
在 nginx 中每个进程会有一个连接数的最大上限，这个上限与系统对 fd 的限制不一样。
在操作系统中，通过 ulimit -n 可以得到一个进程所能够打开的 fd 的最大数，即 nofile。
因为每个 socket 连接会占用掉一个 fd，所以这也会限制进程的最大连接数，当然也会直接影响到程序所能支持的最大并发数。当 fd 用完后，再创建socket时就会失败。
nginx 通过设置 worker_connectons 来设置每个进程支持的最大连接数。
如果该值大于 nofile，那么实际的最大连接数是 nofile，nginx 会有警告。
nginx 在实现时是通过一个连接池来管理的，每个 worker 进程都有一个独立的连接池，连接池的大小是 worker_connections。
这里的连接池里面保存的其实不是真实的连接，它只是一个 worker_connections 大小的一个 ngx_connection_t 结构的数组。
并且 nginx 会通过一个链表 free_connections 来保存所有的空闲 ngx_connection_t，每次获取一个连接时，就从空闲连接链表中获取一个，用完后再放回空闲连接链表里。

&emsp;&emsp;
很多人会误解 worker_connections 这个参数的意思，认为这个值就是 nginx 所能建立连接的最大值。
其实不然，这个值是表示每个 worker 进程所能建立连接的最大值，所以一个nginx能建立的最大连接数应该是 worker_connections x worker_processes。
当然这里说的是最大连接数对于 HTTP 请求本地资源来说，能够支持的最大并发数量是 worker_connections x worker_processes，而如果是 HTTP 作为反向代理，最大并发数量应该是 worker_connections x worker_processes / 2。
因为作为反向代理服务器，每个并发会建立与客户端的连接和与后端服务的连接，会占用两个连接。

&emsp;&emsp;
那么，前面有说过一个客户端连接过来后，多个空闲的进程，会竞争这个连接，很容易看到这种竞争会导致不公平，如果某个进程得到 accept 的机会比较多，它的空闲连接很快就用完了，如果不提前做一些控制，当 accept 到一个新的 tcp 连接后，因为无法得到空闲连接，而且无法将此连接转交给其它进程，最终会导致此 tcp 连接得不到处理，就中止掉了。
很显然这是不公平的，有的进程有空余连接，却没有处理机会，有的进程因为没有空余连接，却人为地丢弃连接。
那么如何解决这个问题呢？
首先 nginx 的处理得先打开 accept_mutex 选项，此时只有获得了 accept_mutex 的进程才会去添加 accept 事件，也就是说 nginx 会控制进程是否添加 accept 事件。
nginx 使用一个叫 ngx_accept_disabled 的变量来控制是否去竞争 accept_mutex 锁。
在第一段代码中，计算 ngx_accept_disabled 的值，这个值是 nginx 单进程的所有连接总数的八分之一，减去剩下的空闲连接数量，得到的这个 ngx_accept_disabled 有一个规律，当剩余连接数小于总连接数的八分之一时，其值才大于0，而且剩余的连接数越小，这个值越大。
再看第二段代码，当 ngx_accept_disabled 大于 0 时，不会去尝试获取 accept_mutex 锁，并且将 ngx_accept_disabled 减 1，于是每次执行到此处时，都会去减 1，直到小于 0。
不去获取 accept_mutex 锁，就是等于让出获取连接的机会，很显然可以看出，当空余连接越少时，ngx_accept_disable 越大，于是让出的机会就越多，这样其它进程获取锁的机会也就越大。
不去 accept，自己的连接就控制下来了，其它进程的连接池就会得到利用，这样 nginx 就控制了多进程间连接的平衡了。

    ngx_accept_disabled = ngx_cycle->connection_n / 8 - ngx_cycle->free_connection_n;

    if (ngx_accept_disabled > 0) {
        ngx_accept_disabled--;
    } else {
        if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {
            return;
        }

        if (ngx_accept_mutex_held) {
            flags |= NGX_POST_EVENTS;
        } else {
            if (timer == NGX_TIMER_INFINITE || timer > ngx_accept_mutex_delay) {
                timer = ngx_accept_mutex_delay;
            }
        }
    }

&emsp;&emsp;
好了连接就先介绍到这，本章的目的是介绍基本概念，知道在 nginx 中连接是个什么东西就行了，而且连接是属于比较高级的用法，在后面的模块开发高级篇会有专门的章节来讲解连接与事件的实现及使用。